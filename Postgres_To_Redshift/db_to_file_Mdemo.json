{
  "jobConfig": {
    "name": "db_to_file_Mdemo",
    "description": "",
    "role": "arn:aws:iam::784201552332:role/adapt-glue-role",
    "command": "glueetl",
    "version": "3.0",
    "workerType": "G.2X",
    "numberOfWorkers": 100,
    "maxCapacity": 200,
    "maxRetries": 0,
    "timeout": 2880,
    "maxConcurrentRuns": 1,
    "security": "none",
    "scriptName": "db_to_file_Mdemo.py",
    "scriptLocation": "s3://aws-glue-assets-784201552332-us-east-2/scripts/",
    "language": "python-3",
    "jobParameters": [
      {
        "key": "--conf",
        "value": "spark.shuffle.glue.s3ShuffleBucket=s3://aws-glue-assets-784201552332-us-east-2/temporary/",
        "existing": false
      },
      {
        "key": "--write-shuffle-files-to-s3",
        "value": "true",
        "existing": false
      },
      {
        "key": "--write-shuffle-spills-to-s3",
        "value": "true",
        "existing": false
      }
    ],
    "tags": [],
    "jobMode": "DEVELOPER_MODE",
    "createdOn": "2023-03-09T05:55:08.037Z",
    "developerMode": true,
    "connectionsList": [
      "adapt-postgres"
    ],
    "temporaryDirectory": "s3://aws-glue-assets-784201552332-us-east-2/temporary/",
    "etlAutoScaling": true,
    "logging": true,
    "glueHiveMetastore": true,
    "etlAutoTuning": true,
    "metrics": true,
    "spark": true,
    "dependentPath": "s3://testadapt/aws-glue-add-modules/spark-avro_2.12-3.1.1.jar",
    "bookmark": "job-bookmark-enable",
    "sparkPath": "s3://aws-glue-assets-784201552332-us-east-2/sparkHistoryLogs/",
    "flexExecution": false,
    "minFlexWorkers": null
  },
  "hasBeenSaved": false,
  "script": "import sys\r\nimport pyspark\r\nfrom pyspark.sql.functions import *\r\nfrom pyspark.sql.functions import col\r\nfrom pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nargs = getResolvedOptions(sys.argv,['TempDir'])\r\n\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\nlogger = glueContext.get_logger()\r\n\r\n# Postgres - Construct JDBC connection options\r\npgconn1 = {\r\n    \"url\": \"jdbc:postgresql://172.31.9.108:5432/demodb\",\r\n    \"dbtable\": \"employee\",\r\n    \"user\": \"postgres\",\r\n    \"password\": \"password1\"}\r\n    \r\npgconn2 = {\r\n    \"url\": \"jdbc:postgresql://172.31.9.108:5432/demodb\",\r\n    \"dbtable\": \"salary\",\r\n    \"user\": \"postgres\",\r\n    \"password\": \"password1\"}\r\n\r\n\r\nlogger.info(\"-------------Reading the data from the source database -------------\")\r\n\r\nfile_format=\"csv\"\r\n\r\nempdf = glueContext.create_dynamic_frame.from_options(connection_type=\"postgresql\",\r\n                                                          connection_options=pgconn1)\r\nsaldf = glueContext.create_dynamic_frame.from_options(connection_type=\"postgresql\",\r\n                                                          connection_options=pgconn2)                                                         \r\npath=\"s3://testadapt/output_pg_to_s3_mdemo/\"                                                      \r\n\r\n    \r\nemp_df=empdf.toDF()   \r\nemp_df.createOrReplaceTempView(\"employee\")\r\n\r\nsal_df=saldf.toDF()   \r\nsal_df.createOrReplaceTempView(\"salary\")\r\n\r\nlogger.info(\"-------------Creating the dataframe -------------\")\r\n#Adding new column emp_experience to the dataframe:\r\ndf1 = spark.sql(\"select e.emp_id,e.emp_name,e.emp_dep,e.emp_dob,e.emp_doj,s.emp_sal from employee e join salary s on e.emp_id=s.emp_id where s.emp_sal>5000;\")\r\n\r\ndf2=df1.withColumn(\"emp_experience\",round(months_between(current_date(),col(\"emp_doj\"))/lit(12),1))\r\ndf2.createOrReplaceTempView(\"recent_joinee\")\r\ndf3=spark.sql(\"select * from recent_joinee where emp_experience>5;\")\r\nlogger.info(\"-------------Writing to S3 -------------\")\r\n\r\nif bool(df3.head(1)):\r\n    df3.coalesce(1).write.option(\"header\",\"true\").format(file_format).mode('overwrite').save(path)\r\n\r\n\r\n"
}
